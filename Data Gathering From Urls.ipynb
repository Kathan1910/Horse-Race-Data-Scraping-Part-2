{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e1e5f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horse Name: Sol Snatcher\n",
      "Age: Age 6\n",
      "Fleet Figure: 47\n",
      "Gender: Mare\n",
      "Stable Name: Shooting Stars\n",
      "Horse Price: 82,000.00\n",
      "Horse Status: Ready to Race!\n",
      "Starts(W-P-S): 36 (4-7-3)\n",
      "Career Earnings: 29,086.62\n",
      "Largest Purse: 9,024.00\n",
      "Career Injuries: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "# Initialize the web driver (Chrome WebDriver should be in your PATH)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Website URL\n",
    "url = 'https://photofinish.live/marketplace/a99022fe-2255-454a-a6e9-0f3375cb20c0'  # Your specific URL here\n",
    "\n",
    "try:\n",
    "    # Navigate to the URL\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for the page to load (you may need to adjust the wait time)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//div[@class=\"flex flex-row gap-2\"]')))\n",
    "\n",
    "    # Get the page source after navigating to the specific link\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # Parse the page source with BeautifulSoup\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # Scrape the desired information\n",
    "    horse_name = soup.find('div', class_='flex flex-row gap-2').find('span', class_='text-2xl').text.strip() if soup.find('div', class_='flex flex-row gap-2') else None\n",
    "    age = soup.find('div', class_='flex flex-row max-w-full whitespace-nowrap sm:text-xs').find('span').text.strip() if soup.find('div', class_='flex flex-row max-w-full whitespace-nowrap sm:text-xs') else None\n",
    "    fleet_figure = soup.find('span', class_='text-xs').text.strip() if soup.find('span', class_='text-xs') else None\n",
    "    # gender = soup.find('div', class_='flex flex-row justify-start items-center text-sky-300').text.strip() if soup.find('div', class_='flex flex-row justify-start items-center text-sky-300') else None\n",
    "    stable_name = soup.find('a', class_='text-secondary-100/80 hover:text-secondary-400 underline block pl-1 cursor-pointer').text.strip() if soup.find('a', class_='text-secondary-100/80 hover:text-secondary-400 underline block pl-1 cursor-pointer') else None\n",
    "    \n",
    "    # Check for gender in the first location\n",
    "    gender_element = soup.find('div', class_='flex flex-row justify-start items-center text-sky-300')\n",
    "    if gender_element:\n",
    "        gender = gender_element.text.strip()\n",
    "    else:\n",
    "    # If not found in the first location, check the second location\n",
    "        gender_element = soup.find('div', class_='flex flex-row justify-start items-center text-pink-300')\n",
    "        if gender_element:\n",
    "            gender = gender_element.text.strip()\n",
    "        else:\n",
    "            gender = None\n",
    "    \n",
    "    # Extracting the price using a regular expression\n",
    "    price_pattern = r'Purchase (\\d+,\\d+\\.\\d+)'\n",
    "    price_match = re.search(price_pattern, page_source)\n",
    "    if price_match:\n",
    "        horse_price = price_match.group(1)\n",
    "    else:\n",
    "        horse_price = None\n",
    "\n",
    "    # Extracting the horse's status\n",
    "    status_elements = soup.find_all('div', class_='flex flex-row') + soup.find_all('p', class_='text-white ml-1')\n",
    "    horse_status = None\n",
    "\n",
    "    for element in status_elements:\n",
    "        if 'Retired' in element.text:\n",
    "            horse_status = 'Retired'\n",
    "            break\n",
    "        elif 'Exhausted' in element.text:\n",
    "            horse_status = 'Exhausted'\n",
    "            break\n",
    "        elif 'Foal' in element.text:\n",
    "            horse_status = 'Foal'\n",
    "            break\n",
    "        elif 'Ready to Race!' in element.text:\n",
    "            horse_status = 'Ready to Race!'\n",
    "        elif 'Pregnant' in element.text:\n",
    "            horse_status = 'Pregnant'\n",
    "            break\n",
    "\n",
    "    # Extracting Starts(W-P-S)\n",
    "    starts_element = soup.find('p', class_='text-sm text-white font-inter')\n",
    "    starts_data = starts_element.text.strip() if starts_element else None\n",
    "\n",
    "    # Example code for extracting career earnings\n",
    "    career_earnings_element = soup.find_all('p', class_='text-sm text-slate-300')\n",
    "    if len(career_earnings_element) >= 1:\n",
    "        career_earnings = career_earnings_element[0].text.strip()\n",
    "    else:\n",
    "        career_earnings = None\n",
    "\n",
    "    # Example code for extracting largest purse\n",
    "    if len(career_earnings_element) >= 2:\n",
    "        largest_purse = career_earnings_element[1].text.strip()\n",
    "    else:\n",
    "        largest_purse = None\n",
    "\n",
    "    # Extracting Career Injuries\n",
    "    injuries_element = soup.find('p', class_='font-inter text-sm text-white')\n",
    "    career_injuries = injuries_element.text.strip() if injuries_element else None\n",
    "\n",
    "    # Print the scraped information\n",
    "    print(\"Horse Name:\", horse_name)\n",
    "    print(\"Age:\", age)\n",
    "    print(\"Fleet Figure:\", fleet_figure)\n",
    "    print(\"Gender:\", gender)\n",
    "    print(\"Stable Name:\", stable_name)\n",
    "    print(\"Horse Price:\", horse_price)\n",
    "    print(\"Horse Status:\", horse_status)\n",
    "    print(\"Starts(W-P-S):\", starts_data)\n",
    "    print(\"Career Earnings:\", career_earnings)\n",
    "    print(\"Largest Purse:\", largest_purse)\n",
    "    print(\"Career Injuries:\", career_injuries)\n",
    "\n",
    "    # Save the scraped data to a DataFrame\n",
    "    scraped_data = pd.DataFrame({\n",
    "        'Horse Name': [horse_name],\n",
    "        'Age': [age],\n",
    "        'Fleet Figure': [fleet_figure],\n",
    "        'Gender': [gender],\n",
    "        'Stable Name': [stable_name],\n",
    "        'Horse Price': [horse_price],\n",
    "        'Horse Status': [horse_status],\n",
    "        'Starts(W-P-S)': [starts_data],\n",
    "        'Career Earnings': [career_earnings],\n",
    "        'Largest Purse': [largest_purse],\n",
    "        'Career Injuries': [career_injuries]\n",
    "    })\n",
    "\n",
    "    # Assign a unique Horse ID\n",
    "    scraped_data['Horse ID'] = 1  # You can assign a different unique ID if needed\n",
    "\n",
    "    # Save the scraped data to a CSV file (customize the filename and path as needed)\n",
    "    scraped_data.to_csv('scraped_data.csv', index=False)\n",
    "\n",
    "    try:\n",
    "        # Scroll to the CSV download button\n",
    "        csv_download_button = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, '//a[contains(text(), \"Download CSV\")]'))\n",
    "        )\n",
    "        csv_download_button.click()\n",
    "\n",
    "        # Wait for the download to complete (you may need to adjust the wait time)\n",
    "        time.sleep(10)  # Adjust as needed\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"CSV not available.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", str(e))\n",
    "\n",
    "finally:\n",
    "    # Close the browser window when done\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10be6d29",
   "metadata": {},
   "source": [
    "## Combine Both The CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55f1f91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horse Name: Sol Snatcher\n",
      "Age: Age 6\n",
      "Fleet Figure: 47\n",
      "Gender: Mare\n",
      "Stable Name: Shooting Stars\n",
      "Horse Price: 82,000.00\n",
      "Horse Status: Ready to Race!\n",
      "Starts(W-P-S): 36 (4-7-3)\n",
      "Career Earnings: 29,086.62\n",
      "Largest Purse: 9,024.00\n",
      "Career Injuries: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "# Initialize the web driver (Chrome WebDriver should be in your PATH)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Website URL\n",
    "url = 'https://photofinish.live/marketplace/a99022fe-2255-454a-a6e9-0f3375cb20c0'  # Your specific URL here\n",
    "\n",
    "try:\n",
    "    # Navigate to the URL\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for the page to load (you may need to adjust the wait time)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//div[@class=\"flex flex-row gap-2\"]')))\n",
    "\n",
    "    # Get the page source after navigating to the specific link\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # Parse the page source with BeautifulSoup\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # Scrape the desired information (same code as before)\n",
    "    horse_name = soup.find('div', class_='flex flex-row gap-2').find('span', class_='text-2xl').text.strip() if soup.find('div', class_='flex flex-row gap-2') else None\n",
    "    age = soup.find('div', class_='flex flex-row max-w-full whitespace-nowrap sm:text-xs').find('span').text.strip() if soup.find('div', class_='flex flex-row max-w-full whitespace-nowrap sm:text-xs') else None\n",
    "    fleet_figure = soup.find('span', class_='text-xs').text.strip() if soup.find('span', class_='text-xs') else None\n",
    "    # gender = soup.find('div', class_='flex flex-row justify-start items-center text-sky-300').text.strip() if soup.find('div', class_='flex flex-row justify-start items-center text-sky-300') else None\n",
    "    stable_name = soup.find('a', class_='text-secondary-100/80 hover:text-secondary-400 underline block pl-1 cursor-pointer').text.strip() if soup.find('a', class_='text-secondary-100/80 hover:text-secondary-400 underline block pl-1 cursor-pointer') else None\n",
    "    \n",
    "    # Check for gender in the first location\n",
    "    gender_element = soup.find('div', class_='flex flex-row justify-start items-center text-sky-300')\n",
    "    if gender_element:\n",
    "        gender = gender_element.text.strip()\n",
    "    else:\n",
    "    # If not found in the first location, check the second location\n",
    "        gender_element = soup.find('div', class_='flex flex-row justify-start items-center text-pink-300')\n",
    "        if gender_element:\n",
    "            gender = gender_element.text.strip()\n",
    "        else:\n",
    "            gender = None\n",
    "    \n",
    "    # Extracting the price using a regular expression\n",
    "    price_pattern = r'Purchase (\\d+,\\d+\\.\\d+)'\n",
    "    price_match = re.search(price_pattern, page_source)\n",
    "    if price_match:\n",
    "        horse_price = price_match.group(1)\n",
    "    else:\n",
    "        horse_price = None\n",
    "\n",
    "    # Extracting the horse's status\n",
    "    status_elements = soup.find_all('div', class_='flex flex-row') + soup.find_all('p', class_='text-white ml-1')\n",
    "    horse_status = None\n",
    "\n",
    "    for element in status_elements:\n",
    "        if 'Retired' in element.text:\n",
    "            horse_status = 'Retired'\n",
    "            break\n",
    "        elif 'Exhausted' in element.text:\n",
    "            horse_status = 'Exhausted'\n",
    "            break\n",
    "        elif 'Foal' in element.text:\n",
    "            horse_status = 'Foal'\n",
    "            break\n",
    "        elif 'Ready to Race!' in element.text:\n",
    "            horse_status = 'Ready to Race!'\n",
    "        elif 'Pregnant' in element.text:\n",
    "            horse_status = 'Pregnant'\n",
    "            break\n",
    "\n",
    "    # Extracting Starts(W-P-S)\n",
    "    starts_element = soup.find('p', class_='text-sm text-white font-inter')\n",
    "    starts_data = starts_element.text.strip() if starts_element else None\n",
    "\n",
    "    # Example code for extracting career earnings\n",
    "    career_earnings_element = soup.find_all('p', class_='text-sm text-slate-300')\n",
    "    if len(career_earnings_element) >= 1:\n",
    "        career_earnings = career_earnings_element[0].text.strip()\n",
    "    else:\n",
    "        career_earnings = None\n",
    "\n",
    "    # Example code for extracting largest purse\n",
    "    if len(career_earnings_element) >= 2:\n",
    "        largest_purse = career_earnings_element[1].text.strip()\n",
    "    else:\n",
    "        largest_purse = None\n",
    "\n",
    "    # Extracting Career Injuries\n",
    "    injuries_element = soup.find('p', class_='font-inter text-sm text-white')\n",
    "    career_injuries = injuries_element.text.strip() if injuries_element else None\n",
    "\n",
    "    # Print the scraped information\n",
    "    print(\"Horse Name:\", horse_name)\n",
    "    print(\"Age:\", age)\n",
    "    print(\"Fleet Figure:\", fleet_figure)\n",
    "    print(\"Gender:\", gender)\n",
    "    print(\"Stable Name:\", stable_name)\n",
    "    print(\"Horse Price:\", horse_price)\n",
    "    print(\"Horse Status:\", horse_status)\n",
    "    print(\"Starts(W-P-S):\", starts_data)\n",
    "    print(\"Career Earnings:\", career_earnings)\n",
    "    print(\"Largest Purse:\", largest_purse)\n",
    "    print(\"Career Injuries:\", career_injuries)\n",
    "\n",
    "    # Save the scraped data to a DataFrame\n",
    "    scraped_data = pd.DataFrame({\n",
    "        'Horse Name': [horse_name],\n",
    "        'Age': [age],\n",
    "        'Fleet Figure': [fleet_figure],\n",
    "        'Gender': [gender],\n",
    "        'Stable Name': [stable_name],\n",
    "        'Horse Price': [horse_price],\n",
    "        'Horse Status': [horse_status],\n",
    "        'Starts(W-P-S)': [starts_data],\n",
    "        'Career Earnings': [career_earnings],\n",
    "        'Largest Purse': [largest_purse],\n",
    "        'Career Injuries': [career_injuries]\n",
    "    })\n",
    "\n",
    "    # Assign a unique Horse ID\n",
    "    scraped_data['Horse ID'] = 1  # You can assign a different unique ID if needed\n",
    "\n",
    "    # Save the scraped data to a CSV file (customize the filename and path as needed)\n",
    "    scraped_data.to_csv('scraped_data.csv', index=False)\n",
    "    \n",
    "    # Scroll to the CSV download button\n",
    "    csv_download_button = WebDriverWait(driver, 10).until(\n",
    "    EC.element_to_be_clickable((By.XPATH, '//a[contains(text(), \"Download CSV\")]'))\n",
    "    )\n",
    "    csv_download_button.click()\n",
    "\n",
    "    # Wait for the download to complete (you may need to adjust the wait time)\n",
    "    time.sleep(10)  # Adjust as needed\n",
    "\n",
    "    # Load the scraped horse data\n",
    "    scraped_horse_data = pd.read_csv('scraped_data.csv')\n",
    "\n",
    "    # Load the downloaded race data\n",
    "    downloaded_race_data = pd.read_csv(r'C:\\Users\\kathan\\Downloads\\completed-races.csv')\n",
    "\n",
    "    # Repeat scraped horse data for each row of race data\n",
    "    repeated_horse_data = pd.concat([scraped_horse_data] * len(downloaded_race_data), ignore_index=True)\n",
    "\n",
    "    # Combine the two DataFrames column-wise\n",
    "    combined_data = pd.concat([downloaded_race_data, repeated_horse_data], axis=1)\n",
    "\n",
    "    # Save the combined data to a new CSV file\n",
    "    combined_data.to_csv('combined_data.csv', index=False)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", str(e))\n",
    "\n",
    "finally:\n",
    "    # Close the browser window when done\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceccb8a8",
   "metadata": {},
   "source": [
    "## Change File Location Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e65079c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'completed-races.csv' moved successfully to 'E:\\Web Scraping\\Login Jupyter\\temp'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Set the path to your default download directory\n",
    "default_download_dir = r'C:\\Users\\kathan\\Downloads'  # Replace YourUsername with your actual username\n",
    "\n",
    "# Set the path to your desired directory\n",
    "desired_directory = r'E:\\Web Scraping\\Login Jupyter\\temp'\n",
    "\n",
    "# Specify the file name you want to move\n",
    "file_to_move = 'completed-races.csv'\n",
    "\n",
    "# Check if the file exists in the default download directory\n",
    "if os.path.exists(os.path.join(default_download_dir, file_to_move)):\n",
    "    # Create the full source path and destination path\n",
    "    source_path = os.path.join(default_download_dir, file_to_move)\n",
    "    destination_path = os.path.join(desired_directory, file_to_move)\n",
    "\n",
    "    # Move the file from the default download directory to the desired directory\n",
    "    try:\n",
    "        shutil.move(source_path, destination_path)\n",
    "        print(f\"File '{file_to_move}' moved successfully to '{desired_directory}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error moving the file: {e}\")\n",
    "else:\n",
    "    print(f\"File '{file_to_move}' not found in the default download directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12939209",
   "metadata": {},
   "source": [
    "## Combination Of Both Functionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a05ba70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horse Name: Sol Snatcher\n",
      "Age: Age 6\n",
      "Fleet Figure: 47\n",
      "Gender: Mare\n",
      "Stable Name: Shooting Stars\n",
      "Horse Price: 82,000.00\n",
      "Horse Status: Ready to Race!\n",
      "Starts(W-P-S): 36 (4-7-3)\n",
      "Career Earnings: 29,086.62\n",
      "Largest Purse: 9,024.00\n",
      "Career Injuries: 0\n",
      "File 'completed-races.csv' moved successfully to 'E:\\Web Scraping\\Login Jupyter\\temp'.\n",
      "Combined data saved to 'E:\\Web Scraping\\Login Jupyter\\final'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "# Set the path to your default download directory\n",
    "default_download_dir = r'C:\\Users\\kathan\\Downloads'  # Replace with your actual username\n",
    "\n",
    "# Set the path to your desired directory\n",
    "desired_directory = r'E:\\Web Scraping\\Login Jupyter\\temp'\n",
    "\n",
    "# Specify the file name you want to move and combine\n",
    "file_to_move = 'completed-races.csv'\n",
    "\n",
    "# Initialize the web driver (Chrome WebDriver should be in your PATH)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Website URL\n",
    "url = 'https://photofinish.live/marketplace/a99022fe-2255-454a-a6e9-0f3375cb20c0'  # Your specific URL here\n",
    "\n",
    "try:\n",
    "    # Navigate to the URL\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for the page to load (you may need to adjust the wait time)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//div[@class=\"flex flex-row gap-2\"]')))\n",
    "\n",
    "    # Get the page source after navigating to the specific link\n",
    "    page_source = driver.page_source\n",
    "    \n",
    "     # Parse the page source with BeautifulSoup\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # Scrape the desired information (same code as before)\n",
    "    horse_name = soup.find('div', class_='flex flex-row gap-2').find('span', class_='text-2xl').text.strip() if soup.find('div', class_='flex flex-row gap-2') else None\n",
    "    age = soup.find('div', class_='flex flex-row max-w-full whitespace-nowrap sm:text-xs').find('span').text.strip() if soup.find('div', class_='flex flex-row max-w-full whitespace-nowrap sm:text-xs') else None\n",
    "    fleet_figure = soup.find('span', class_='text-xs').text.strip() if soup.find('span', class_='text-xs') else None\n",
    "    # gender = soup.find('div', class_='flex flex-row justify-start items-center text-sky-300').text.strip() if soup.find('div', class_='flex flex-row justify-start items-center text-sky-300') else None\n",
    "    stable_name = soup.find('a', class_='text-secondary-100/80 hover:text-secondary-400 underline block pl-1 cursor-pointer').text.strip() if soup.find('a', class_='text-secondary-100/80 hover:text-secondary-400 underline block pl-1 cursor-pointer') else None\n",
    "    \n",
    "    # Check for gender in the first location\n",
    "    gender_element = soup.find('div', class_='flex flex-row justify-start items-center text-sky-300')\n",
    "    if gender_element:\n",
    "        gender = gender_element.text.strip()\n",
    "    else:\n",
    "    # If not found in the first location, check the second location\n",
    "        gender_element = soup.find('div', class_='flex flex-row justify-start items-center text-pink-300')\n",
    "        if gender_element:\n",
    "            gender = gender_element.text.strip()\n",
    "        else:\n",
    "            gender = None\n",
    "    \n",
    "    # Extracting the price using a regular expression\n",
    "    price_pattern = r'Purchase (\\d+,\\d+\\.\\d+)'\n",
    "    price_match = re.search(price_pattern, page_source)\n",
    "    if price_match:\n",
    "        horse_price = price_match.group(1)\n",
    "    else:\n",
    "        horse_price = None\n",
    "\n",
    "    # Extracting the horse's status\n",
    "    status_elements = soup.find_all('div', class_='flex flex-row') + soup.find_all('p', class_='text-white ml-1')\n",
    "    horse_status = None\n",
    "\n",
    "    for element in status_elements:\n",
    "        if 'Retired' in element.text:\n",
    "            horse_status = 'Retired'\n",
    "            break\n",
    "        elif 'Exhausted' in element.text:\n",
    "            horse_status = 'Exhausted'\n",
    "            break\n",
    "        elif 'Foal' in element.text:\n",
    "            horse_status = 'Foal'\n",
    "            break\n",
    "        elif 'Ready to Race!' in element.text:\n",
    "            horse_status = 'Ready to Race!'\n",
    "        elif 'Pregnant' in element.text:\n",
    "            horse_status = 'Pregnant'\n",
    "            break\n",
    "\n",
    "    # Extracting Starts(W-P-S)\n",
    "    starts_element = soup.find('p', class_='text-sm text-white font-inter')\n",
    "    starts_data = starts_element.text.strip() if starts_element else None\n",
    "\n",
    "    # Example code for extracting career earnings\n",
    "    career_earnings_element = soup.find_all('p', class_='text-sm text-slate-300')\n",
    "    if len(career_earnings_element) >= 1:\n",
    "        career_earnings = career_earnings_element[0].text.strip()\n",
    "    else:\n",
    "        career_earnings = None\n",
    "\n",
    "    # Example code for extracting largest purse\n",
    "    if len(career_earnings_element) >= 2:\n",
    "        largest_purse = career_earnings_element[1].text.strip()\n",
    "    else:\n",
    "        largest_purse = None\n",
    "\n",
    "    # Extracting Career Injuries\n",
    "    injuries_element = soup.find('p', class_='font-inter text-sm text-white')\n",
    "    career_injuries = injuries_element.text.strip() if injuries_element else None\n",
    "\n",
    "    # Print the scraped information\n",
    "    print(\"Horse Name:\", horse_name)\n",
    "    print(\"Age:\", age)\n",
    "    print(\"Fleet Figure:\", fleet_figure)\n",
    "    print(\"Gender:\", gender)\n",
    "    print(\"Stable Name:\", stable_name)\n",
    "    print(\"Horse Price:\", horse_price)\n",
    "    print(\"Horse Status:\", horse_status)\n",
    "    print(\"Starts(W-P-S):\", starts_data)\n",
    "    print(\"Career Earnings:\", career_earnings)\n",
    "    print(\"Largest Purse:\", largest_purse)\n",
    "    print(\"Career Injuries:\", career_injuries)\n",
    "\n",
    "    # Save the scraped data to a DataFrame\n",
    "    scraped_data = pd.DataFrame({\n",
    "        'Horse Name': [horse_name],\n",
    "        'Age': [age],\n",
    "        'Fleet Figure': [fleet_figure],\n",
    "        'Gender': [gender],\n",
    "        'Stable Name': [stable_name],\n",
    "        'Horse Price': [horse_price],\n",
    "        'Horse Status': [horse_status],\n",
    "        'Starts(W-P-S)': [starts_data],\n",
    "        'Career Earnings': [career_earnings],\n",
    "        'Largest Purse': [largest_purse],\n",
    "        'Career Injuries': [career_injuries]\n",
    "    })\n",
    "\n",
    "    # Assign a unique Horse ID\n",
    "    scraped_data['Horse ID'] = 1  # You can assign a different unique ID if needed\n",
    "\n",
    "    # Save the scraped data to a CSV file (customize the filename and path as needed)\n",
    "    scraped_data.to_csv('scraped_data.csv', index=False)\n",
    "    \n",
    "    # Scroll to the CSV download button\n",
    "    csv_download_button = WebDriverWait(driver, 10).until(\n",
    "    EC.element_to_be_clickable((By.XPATH, '//a[contains(text(), \"Download CSV\")]'))\n",
    "    )\n",
    "    csv_download_button.click()\n",
    "\n",
    "    # Wait for the download to complete (you may need to adjust the wait time)\n",
    "    time.sleep(10)  # Adjust as needed\n",
    "    \n",
    "    # Move the downloaded file to the desired directory\n",
    "    if os.path.exists(os.path.join(default_download_dir, file_to_move)):\n",
    "        source_path = os.path.join(default_download_dir, file_to_move)\n",
    "        destination_path = os.path.join(desired_directory, file_to_move)\n",
    "\n",
    "        try:\n",
    "            shutil.move(source_path, destination_path)\n",
    "            print(f\"File '{file_to_move}' moved successfully to '{desired_directory}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error moving the file: {e}\")\n",
    "    else:\n",
    "        print(f\"File '{file_to_move}' not found in the default download directory.\")\n",
    "\n",
    "    # Load the downloaded race data from the temp directory\n",
    "    downloaded_race_data = pd.read_csv(os.path.join(desired_directory, file_to_move))\n",
    "\n",
    "    # Load the scraped horse data (modify the path as needed)\n",
    "    scraped_horse_data = pd.read_csv('scraped_data.csv')\n",
    "\n",
    "    # Repeat scraped horse data for each row of race data\n",
    "    repeated_horse_data = pd.concat([scraped_horse_data] * len(downloaded_race_data), ignore_index=True)\n",
    "\n",
    "    # Combine the two DataFrames column-wise\n",
    "    combined_data = pd.concat([downloaded_race_data, repeated_horse_data], axis=1)\n",
    "\n",
    "    # Set the path to the final directory\n",
    "    final_directory = r'E:\\Web Scraping\\Login Jupyter\\final'\n",
    "\n",
    "    # Save the combined data to the final directory\n",
    "    combined_data.to_csv(os.path.join(final_directory, 'combined_data.csv'), index=False)\n",
    "    print(f\"Combined data saved to '{final_directory}'.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", str(e))\n",
    "\n",
    "finally:\n",
    "    # Close the browser window when done\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81836a8",
   "metadata": {},
   "source": [
    "## Multiple Horse Links "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff822b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horse Name: Sol Snatcher\n",
      "Age: Age 6\n",
      "Fleet Figure: 47\n",
      "Gender: Mare\n",
      "Stable Name: Shooting Stars\n",
      "Horse Price: 82,000.00\n",
      "Horse Status: Ready to Race!\n",
      "Starts(W-P-S): 36 (4-7-3)\n",
      "Career Earnings: 29,086.62\n",
      "Largest Purse: 9,024.00\n",
      "Career Injuries: 0\n",
      "File 'completed-races.csv' moved successfully to 'E:\\Web Scraping\\Login Jupyter\\temp'.\n",
      "Horse Name: Slow Clap\n",
      "Age: Age 2\n",
      "Fleet Figure: 40\n",
      "Gender: Colt\n",
      "Stable Name: Alen ibro\n",
      "Horse Price: 28,950.00\n",
      "Horse Status: Ready to Race!\n",
      "Starts(W-P-S): 16 (1-3-0)\n",
      "Career Earnings: 2,893.68\n",
      "Largest Purse: 1,373.76\n",
      "Career Injuries: 0\n",
      "File 'completed-races.csv' moved successfully to 'E:\\Web Scraping\\Login Jupyter\\temp'.\n",
      "Horse Name: Foal of Online Oathkeeper\n",
      "Age: Foal\n",
      "Fleet Figure: 00\n",
      "Gender: Filly\n",
      "Stable Name: RVA Racing\n",
      "Horse Price: 65,000.00\n",
      "Horse Status: Foal\n",
      "Starts(W-P-S): 0 (0-0-0)\n",
      "Career Earnings: 0.00\n",
      "Largest Purse: 0.00\n",
      "Career Injuries: None\n",
      "CSV not available for https://photofinish.live/marketplace/a998e4bb-580d-4904-8523-6050b68da8bd. Adding scraped data directly to combined data CSV...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set the path to your default download directory\n",
    "default_download_dir = r'C:\\Users\\kathan\\Downloads'  # Replace with your actual username\n",
    "\n",
    "# Set the path to your desired directory\n",
    "desired_directory = r'E:\\Web Scraping\\Login Jupyter\\temp'\n",
    "\n",
    "# Initialize the web driver (Chrome WebDriver should be in your PATH)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Website URLs for horse links\n",
    "horse_links = [\n",
    "    'https://photofinish.live/marketplace/a99022fe-2255-454a-a6e9-0f3375cb20c0',\n",
    "    'https://photofinish.live/marketplace/76a6937d-cebe-4466-a3e9-41f86d8a07cc',\n",
    "    'https://photofinish.live/marketplace/a998e4bb-580d-4904-8523-6050b68da8bd'\n",
    "    # Add more horse links as needed\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Initialize a unique horse ID counter\n",
    "    horse_id_counter = 1\n",
    "\n",
    "    for horse_link in horse_links:\n",
    "        # Navigate to the horse link\n",
    "        driver.get(horse_link)\n",
    "\n",
    "        # Wait for the page to load (you may need to adjust the wait time)\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//div[@class=\"flex flex-row gap-2\"]')))\n",
    "\n",
    "        # Get the page source after navigating to the specific link\n",
    "        page_source = driver.page_source\n",
    "\n",
    "        # Parse the page source with BeautifulSoup\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # Scrape the desired information\n",
    "        horse_name = soup.find('div', class_='flex flex-row gap-2').find('span', class_='text-2xl').text.strip() if soup.find('div', class_='flex flex-row gap-2') else None\n",
    "        age = soup.find('div', class_='flex flex-row max-w-full whitespace-nowrap sm:text-xs').find('span').text.strip() if soup.find('div', class_='flex flex-row max-w-full whitespace-nowrap sm:text-xs') else None\n",
    "        fleet_figure = soup.find('span', class_='text-xs').text.strip() if soup.find('span', class_='text-xs') else None\n",
    "        stable_name = soup.find('a', class_='text-secondary-100/80 hover:text-secondary-400 underline block pl-1 cursor-pointer').text.strip() if soup.find('a', class_='text-secondary-100/80 hover:text-secondary-400 underline block pl-1 cursor-pointer') else None\n",
    "        \n",
    "        gender_element = soup.find('div', class_='flex flex-row justify-start items-center text-sky-300')\n",
    "        if gender_element:\n",
    "            gender = gender_element.text.strip()\n",
    "        else:\n",
    "            gender_element = soup.find('div', class_='flex flex-row justify-start items-center text-pink-300')\n",
    "            if gender_element:\n",
    "                gender = gender_element.text.strip()\n",
    "            else:\n",
    "                gender = None\n",
    "        \n",
    "        price_pattern = r'Purchase (\\d+,\\d+\\.\\d+)'\n",
    "        price_match = re.search(price_pattern, page_source)\n",
    "        if price_match:\n",
    "            horse_price = price_match.group(1)\n",
    "        else:\n",
    "            horse_price = None\n",
    "\n",
    "        status_elements = soup.find_all('div', class_='flex flex-row') + soup.find_all('p', class_='text-white ml-1')\n",
    "        horse_status = None\n",
    "\n",
    "        for element in status_elements:\n",
    "            if 'Retired' in element.text:\n",
    "                horse_status = 'Retired'\n",
    "                break\n",
    "            elif 'Exhausted' in element.text:\n",
    "                horse_status = 'Exhausted'\n",
    "                break\n",
    "            elif 'Foal' in element.text:\n",
    "                horse_status = 'Foal'\n",
    "                break\n",
    "            elif 'Ready to Race!' in element.text:\n",
    "                horse_status = 'Ready to Race!'\n",
    "            elif 'Pregnant' in element.text:\n",
    "                horse_status = 'Pregnant'\n",
    "                break\n",
    "\n",
    "        starts_element = soup.find('p', class_='text-sm text-white font-inter')\n",
    "        starts_data = starts_element.text.strip() if starts_element else None\n",
    "\n",
    "        career_earnings_element = soup.find_all('p', class_='text-sm text-slate-300')\n",
    "        if len(career_earnings_element) >= 1:\n",
    "            career_earnings = career_earnings_element[0].text.strip()\n",
    "        else:\n",
    "            career_earnings = None\n",
    "\n",
    "        if len(career_earnings_element) >= 2:\n",
    "            largest_purse = career_earnings_element[1].text.strip()\n",
    "        else:\n",
    "            largest_purse = None\n",
    "\n",
    "        injuries_element = soup.find('p', class_='font-inter text-sm text-white')\n",
    "        career_injuries = injuries_element.text.strip() if injuries_element else None\n",
    "\n",
    "        # Print the scraped information\n",
    "        print(\"Horse Name:\", horse_name)\n",
    "        print(\"Age:\", age)\n",
    "        print(\"Fleet Figure:\", fleet_figure)\n",
    "        print(\"Gender:\", gender)\n",
    "        print(\"Stable Name:\", stable_name)\n",
    "        print(\"Horse Price:\", horse_price)\n",
    "        print(\"Horse Status:\", horse_status)\n",
    "        print(\"Starts(W-P-S):\", starts_data)\n",
    "        print(\"Career Earnings:\", career_earnings)\n",
    "        print(\"Largest Purse:\", largest_purse)\n",
    "        print(\"Career Injuries:\", career_injuries)\n",
    "\n",
    "        try:\n",
    "            # Scroll to the CSV download button\n",
    "            csv_download_button = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, '//a[contains(text(), \"Download CSV\")]'))\n",
    "            )\n",
    "            csv_download_button.click()\n",
    "\n",
    "            # Wait for the download to complete (you may need to adjust the wait time)\n",
    "            time.sleep(10)  # Adjust as needed\n",
    "\n",
    "            # Move the downloaded file to the desired directory\n",
    "            if os.path.exists(os.path.join(default_download_dir, file_to_move)):\n",
    "                source_path = os.path.join(default_download_dir, file_to_move)\n",
    "                destination_path = os.path.join(desired_directory, file_to_move)\n",
    "\n",
    "                try:\n",
    "                    shutil.move(source_path, destination_path)\n",
    "                    print(f\"File '{file_to_move}' moved successfully to '{desired_directory}'.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error moving the file: {e}\")\n",
    "            else:\n",
    "                print(f\"File '{file_to_move}' not found in the default download directory.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"CSV not available for {horse_link}. Adding scraped data directly to combined data CSV...\")\n",
    "\n",
    "            # Assign a unique Horse ID\n",
    "            horse_id = horse_id_counter\n",
    "            horse_id_counter += 1\n",
    "\n",
    "            # Create a DataFrame for the scraped horse data\n",
    "            scraped_data = pd.DataFrame({\n",
    "                'Horse ID': [horse_id],\n",
    "                'Horse Name': [horse_name],\n",
    "                'Age': [age],\n",
    "                'Fleet Figure': [fleet_figure],\n",
    "                'Gender': [gender],\n",
    "                'Stable Name': [stable_name],\n",
    "                'Horse Price': [horse_price],\n",
    "                'Horse Status': [horse_status],\n",
    "                'Starts(W-P-S)': [starts_data],\n",
    "                'Career Earnings': [career_earnings],\n",
    "                'Largest Purse': [largest_purse],\n",
    "                'Career Injuries': [career_injuries]\n",
    "            })\n",
    "\n",
    "            # Save the scraped data to a CSV file (customize the filename and path as needed)\n",
    "            scraped_data.to_csv(os.path.join(desired_directory, f'scraped_data_{horse_id}.csv'), index=False)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", str(e))\n",
    "\n",
    "finally:\n",
    "    # Close the browser window when done\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0de16c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horse Name: Sol Snatcher\n",
      "Age: Age 6\n",
      "Fleet Figure: 47\n",
      "Gender: Mare\n",
      "Stable Name: Shooting Stars\n",
      "Horse Price: 82,000.00\n",
      "Horse Status: Ready to Race!\n",
      "Starts(W-P-S): 36 (4-7-3)\n",
      "Career Earnings: 29,086.62\n",
      "Largest Purse: 9,024.00\n",
      "Career Injuries: 0\n",
      "File 'completed-races.csv' moved successfully to 'E:\\Web Scraping\\Login Jupyter\\temp'.\n",
      "Combined data saved to 'E:\\Web Scraping\\Login Jupyter\\final'.\n",
      "Horse Name: Byte Baron\n",
      "Age: Age 6\n",
      "Fleet Figure: 59\n",
      "Gender: Stallion\n",
      "Stable Name: OnlyFoalsAndHorses\n",
      "Horse Price: 9,200.00\n",
      "Horse Status: Ready to Race!\n",
      "Starts(W-P-S): 15 (1-2-0)\n",
      "Career Earnings: 1,692.00\n",
      "Largest Purse: 648.00\n",
      "Career Injuries: 0\n",
      "File 'completed-races.csv' moved successfully to 'E:\\Web Scraping\\Login Jupyter\\temp'.\n",
      "Combined data saved to 'E:\\Web Scraping\\Login Jupyter\\final'.\n",
      "Horse Name: Foal of Online Oathkeeper\n",
      "Age: Foal\n",
      "Fleet Figure: 00\n",
      "Gender: Filly\n",
      "Stable Name: RVA Racing\n",
      "Horse Price: 65,000.00\n",
      "Horse Status: Foal\n",
      "Starts(W-P-S): 0 (0-0-0)\n",
      "Career Earnings: 0.00\n",
      "Largest Purse: 0.00\n",
      "Career Injuries: None\n",
      "An error occurred: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF60A5652A2+57122]\n",
      "\t(No symbol) [0x00007FF60A4DEA92]\n",
      "\t(No symbol) [0x00007FF60A3AE3AB]\n",
      "\t(No symbol) [0x00007FF60A3E7D3E]\n",
      "\t(No symbol) [0x00007FF60A3E7E2C]\n",
      "\t(No symbol) [0x00007FF60A420B67]\n",
      "\t(No symbol) [0x00007FF60A40701F]\n",
      "\t(No symbol) [0x00007FF60A41EB82]\n",
      "\t(No symbol) [0x00007FF60A406DB3]\n",
      "\t(No symbol) [0x00007FF60A3DD2B1]\n",
      "\t(No symbol) [0x00007FF60A3DE494]\n",
      "\tGetHandleVerifier [0x00007FF60A80EF82+2849794]\n",
      "\tGetHandleVerifier [0x00007FF60A861D24+3189156]\n",
      "\tGetHandleVerifier [0x00007FF60A85ACAF+3160367]\n",
      "\tGetHandleVerifier [0x00007FF60A5F6D06+653702]\n",
      "\t(No symbol) [0x00007FF60A4EA208]\n",
      "\t(No symbol) [0x00007FF60A4E62C4]\n",
      "\t(No symbol) [0x00007FF60A4E63F6]\n",
      "\t(No symbol) [0x00007FF60A4D67A3]\n",
      "\tBaseThreadInitThunk [0x00007FFBA3D57344+20]\n",
      "\tRtlUserThreadStart [0x00007FFBA5A226B1+33]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Set the path to your default download directory\n",
    "default_download_dir = r'C:\\Users\\kathan\\Downloads'  # Replace with your actual username\n",
    "\n",
    "# Set the path to your desired directory\n",
    "desired_directory = r'E:\\Web Scraping\\Login Jupyter\\temp'\n",
    "\n",
    "# Initialize the web driver (Chrome WebDriver should be in your PATH)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Website URLs for horse links\n",
    "horse_links = [\n",
    "    'https://photofinish.live/marketplace/a99022fe-2255-454a-a6e9-0f3375cb20c0',\n",
    "    'https://photofinish.live/marketplace/047ba1e1-860c-47d8-ab29-e2ea1882242a',\n",
    "    'https://photofinish.live/marketplace/a998e4bb-580d-4904-8523-6050b68da8bd'\n",
    "    # Add more horse links as needed\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Initialize a unique horse ID counter\n",
    "    horse_id_counter = 1\n",
    "\n",
    "    for horse_link in horse_links:\n",
    "        # Navigate to the horse link\n",
    "        driver.get(horse_link)\n",
    "\n",
    "        # Wait for the page to load (you may need to adjust the wait time)\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//div[@class=\"flex flex-row gap-2\"]')))\n",
    "\n",
    "        # Get the page source after navigating to the specific link\n",
    "        page_source = driver.page_source\n",
    "\n",
    "        # Check if CSV is available, if not, skip this URL\n",
    "        if 'CSV not available' in page_source:\n",
    "            print(\"CSV not available for\", horse_link)\n",
    "            continue\n",
    "\n",
    "        # Parse the page source with BeautifulSoup\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # Scrape the desired information (same code as before)\n",
    "        horse_name = soup.find('div', class_='flex flex-row gap-2').find('span', class_='text-2xl').text.strip() if soup.find('div', class_='flex flex-row gap-2') else None\n",
    "        age = soup.find('div', class_='flex flex-row max-w-full whitespace-nowrap sm:text-xs').find('span').text.strip() if soup.find('div', class_='flex flex-row max-w-full whitespace-nowrap sm:text-xs') else None\n",
    "        fleet_figure = soup.find('span', class_='text-xs').text.strip() if soup.find('span', class_='text-xs') else None\n",
    "        stable_name = soup.find('a', class_='text-secondary-100/80 hover:text-secondary-400 underline block pl-1 cursor-pointer').text.strip() if soup.find('a', class_='text-secondary-100/80 hover:text-secondary-400 underline block pl-1 cursor-pointer') else None\n",
    "        \n",
    "        gender_element = soup.find('div', class_='flex flex-row justify-start items-center text-sky-300')\n",
    "        if gender_element:\n",
    "            gender = gender_element.text.strip()\n",
    "        else:\n",
    "            gender_element = soup.find('div', class_='flex flex-row justify-start items-center text-pink-300')\n",
    "            if gender_element:\n",
    "                gender = gender_element.text.strip()\n",
    "            else:\n",
    "                gender = None\n",
    "        \n",
    "        price_pattern = r'Purchase (\\d+,\\d+\\.\\d+)'\n",
    "        price_match = re.search(price_pattern, page_source)\n",
    "        if price_match:\n",
    "            horse_price = price_match.group(1)\n",
    "        else:\n",
    "            horse_price = None\n",
    "\n",
    "        status_elements = soup.find_all('div', class_='flex flex-row') + soup.find_all('p', class_='text-white ml-1')\n",
    "        horse_status = None\n",
    "\n",
    "        for element in status_elements:\n",
    "            if 'Retired' in element.text:\n",
    "                horse_status = 'Retired'\n",
    "                break\n",
    "            elif 'Exhausted' in element.text:\n",
    "                horse_status = 'Exhausted'\n",
    "                break\n",
    "            elif 'Foal' in element.text:\n",
    "                horse_status = 'Foal'\n",
    "                break\n",
    "            elif 'Ready to Race!' in element.text:\n",
    "                horse_status = 'Ready to Race!'\n",
    "            elif 'Pregnant' in element.text:\n",
    "                horse_status = 'Pregnant'\n",
    "                break\n",
    "\n",
    "        starts_element = soup.find('p', class_='text-sm text-white font-inter')\n",
    "        starts_data = starts_element.text.strip() if starts_element else None\n",
    "\n",
    "        career_earnings_element = soup.find_all('p', class_='text-sm text-slate-300')\n",
    "        if len(career_earnings_element) >= 1:\n",
    "            career_earnings = career_earnings_element[0].text.strip()\n",
    "        else:\n",
    "            career_earnings = None\n",
    "\n",
    "        if len(career_earnings_element) >= 2:\n",
    "            largest_purse = career_earnings_element[1].text.strip()\n",
    "        else:\n",
    "            largest_purse = None\n",
    "\n",
    "        injuries_element = soup.find('p', class_='font-inter text-sm text-white')\n",
    "        career_injuries = injuries_element.text.strip() if injuries_element else None\n",
    "\n",
    "        # Print the scraped information\n",
    "        print(\"Horse Name:\", horse_name)\n",
    "        print(\"Age:\", age)\n",
    "        print(\"Fleet Figure:\", fleet_figure)\n",
    "        print(\"Gender:\", gender)\n",
    "        print(\"Stable Name:\", stable_name)\n",
    "        print(\"Horse Price:\", horse_price)\n",
    "        print(\"Horse Status:\", horse_status)\n",
    "        print(\"Starts(W-P-S):\", starts_data)\n",
    "        print(\"Career Earnings:\", career_earnings)\n",
    "        print(\"Largest Purse:\", largest_purse)\n",
    "        print(\"Career Injuries:\", career_injuries)\n",
    "        \n",
    "\n",
    "        # Save the scraped data to a DataFrame\n",
    "        scraped_data = pd.DataFrame({\n",
    "            'Horse Name': [horse_name],\n",
    "            'Age': [age],\n",
    "            'Fleet Figure': [fleet_figure],\n",
    "            'Gender': [gender],\n",
    "            'Stable Name': [stable_name],\n",
    "            'Horse Price': [horse_price],\n",
    "            'Horse Status': [horse_status],\n",
    "            'Starts(W-P-S)': [starts_data],\n",
    "            'Career Earnings': [career_earnings],\n",
    "            'Largest Purse': [largest_purse],\n",
    "            'Career Injuries': [career_injuries]\n",
    "        })\n",
    "\n",
    "        # Assign a unique Horse ID\n",
    "        scraped_data['Horse ID'] = horse_id_counter\n",
    "        horse_id_counter += 1\n",
    "\n",
    "        # Save the scraped data to a CSV file (customize the filename and path as needed)\n",
    "        scraped_data.to_csv(f'scraped_data_{horse_id_counter}.csv', index=False)\n",
    "\n",
    "        # Scroll to the CSV download button\n",
    "        csv_download_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, '//a[contains(text(), \"Download CSV\")]'))\n",
    "        )\n",
    "        csv_download_button.click()\n",
    "\n",
    "        # Wait for the download to complete (you may need to adjust the wait time)\n",
    "        time.sleep(10)  # Adjust as needed\n",
    "\n",
    "        # Move the downloaded file to the desired directory\n",
    "        if os.path.exists(os.path.join(default_download_dir, file_to_move)):\n",
    "            source_path = os.path.join(default_download_dir, file_to_move)\n",
    "            destination_path = os.path.join(desired_directory, file_to_move)\n",
    "\n",
    "            try:\n",
    "                shutil.move(source_path, destination_path)\n",
    "                print(f\"File '{file_to_move}' moved successfully to '{desired_directory}'.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error moving the file: {e}\")\n",
    "        else:\n",
    "            print(f\"File '{file_to_move}' not found in the default download directory.\")\n",
    "\n",
    "        # Load the downloaded race data from the temp directory\n",
    "        downloaded_race_data = pd.read_csv(os.path.join(desired_directory, file_to_move))\n",
    "\n",
    "        # Load the scraped horse data (modify the path as needed)\n",
    "        scraped_horse_data = pd.read_csv(f'scraped_data_{horse_id_counter}.csv')\n",
    "\n",
    "        # Repeat scraped horse data for each row of race data\n",
    "        repeated_horse_data = pd.concat([scraped_horse_data] * len(downloaded_race_data), ignore_index=True)\n",
    "\n",
    "        # Combine the two DataFrames column-wise\n",
    "        combined_data = pd.concat([downloaded_race_data, repeated_horse_data], axis=1)\n",
    "\n",
    "        # Set the path to the final directory\n",
    "        final_directory = r'E:\\Web Scraping\\Login Jupyter\\final'\n",
    "\n",
    "        # Save the combined data to the final directory\n",
    "        combined_data.to_csv(os.path.join(final_directory, f'combined_data_{horse_id_counter}.csv'), index=False)\n",
    "        print(f\"Combined data saved to '{final_directory}'.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", str(e))\n",
    "\n",
    "finally:\n",
    "    # Close the browser window when done\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e8bb1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "mynev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
